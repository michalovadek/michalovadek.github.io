<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>time series on Michal Ovádek</title>
    <link>/tags/time-series/</link>
    <description>Recent content in time series on Michal Ovádek</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2021 Michal Ovádek</copyright>
    <lastBuildDate>Sat, 07 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/time-series/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kernel density estimation for time series plots</title>
      <link>/post/time-series-data-and-kernel-density/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/time-series-data-and-kernel-density/</guid>
      <description>


&lt;p&gt;In my research I frequently encounter count data that has a temporal dimension. The standard way of visualizing such data is to take the counts grouped at some temporal level (month, year, etc). The clustering essentially does away with the problem of the y-axis, namely in that it creates variation at evenly spaced out x-axis intervals. Let’s generate randomly a bunch of dates and show an archetypal bar plot at the year level of, say, judgments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)

set.seed(92139)

dates &amp;lt;- as_date(as.Date(&amp;quot;2000-01-01&amp;quot;):as.Date(&amp;quot;2020-01-01&amp;quot;))

dat &amp;lt;- sample(dates, 300, replace = F,
              prob = abs(rnorm(length(dates),100, 200))
              )

dat %&amp;gt;% 
  enframe() %&amp;gt;% 
  mutate(year = as.integer(str_sub(value,1,4))) %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  count() %&amp;gt;% 
  ggplot(aes(x = year, y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:barplot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2019-12-07-time-series-data-and-kernel-density_files/figure-html/barplot-1.png&#34; alt=&#34;Number of fictitious judgments between 2000-2020&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Number of fictitious judgments between 2000-2020
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Nothing very special here. However, note that instead of using the actual &lt;em&gt;dates&lt;/em&gt; of the judgments, we sacrifice precision to obtain variation on the y-axis. Often the grouping (temporal) level – year in our case – does not carry any particular meaning beyond the fact that years are readily comprehensible to everyone. Can we do better?&lt;/p&gt;
&lt;p&gt;Let’s go back to the same data and visualize the date of each judgment. Without any additional variable that could supply variation along a second dimension, I simply fix &lt;em&gt;y = 1&lt;/em&gt; to denote that each judgment is counted, well, once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  enframe() %&amp;gt;% 
  ggplot(aes(x = value, y = 1)) +
  geom_point(alpha = 0.1) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dots&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2019-12-07-time-series-data-and-kernel-density_files/figure-html/dots-1.png&#34; alt=&#34;Dates of fictitious judgments between 2000-2020&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Dates of fictitious judgments between 2000-2020
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The visualization is not very good. Although the x-axis is now a proper continuous timeseries measured in days, there are too many judgments and days in the 20-year period to see clearly any patterns in the data. Increasing the transparency of the data points is a good first step, as darker areas indicate greater &lt;em&gt;density&lt;/em&gt; of judgments around that time.&lt;/p&gt;
&lt;p&gt;We can exploit the full potential of the dates using the statistical concept of &lt;em&gt;kernel density&lt;/em&gt;. For ease of interpretation, we can think of this as a smoothed histogram. It is very straightforward to implement in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  enframe() %&amp;gt;% 
  ggplot(aes(x = value)) +
  geom_vline(xintercept = as.Date(&amp;quot;2013-01-01&amp;quot;), lty = 2) +
  annotate(&amp;quot;text&amp;quot;, x = as.Date(&amp;quot;2014-12-31&amp;quot;), y = 0.99, label = &amp;quot;Some event&amp;quot;, fontface = &amp;quot;bold&amp;quot;) +
  geom_histogram(alpha = 0.1, aes(y = ..ncount..), bins = 40) +
  stat_density(geom = &amp;quot;line&amp;quot;, adjust = 1/10, kernel = &amp;quot;gaussian&amp;quot;, aes(y = ..scaled..)) +
  geom_point(alpha = 0.1, y = 0.1) +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  labs(x = NULL, y = &amp;quot;Kernel density estimates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:kde&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2019-12-07-time-series-data-and-kernel-density_files/figure-html/kde-1.png&#34; alt=&#34;Dates of fictitious judgments between 2000-2020 with density estimated&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Dates of fictitious judgments between 2000-2020 with density estimated
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using &lt;code&gt;stat_density&lt;/code&gt; we get a nice continuous (estimated) timeseries of our data. The idea is to basically treat the time dimension as just a series of numbers over which a continuous distribution of our data can be drawn.&lt;/p&gt;
&lt;p&gt;Setting &lt;code&gt;y = ..scaled..&lt;/code&gt; scales density to the 0-1 range, which is useful if we want to, for example, compare the kernel density estimates with a histogram that we similarly scale with &lt;code&gt;y = ..ncount..&lt;/code&gt;. Changing &lt;code&gt;adjust&lt;/code&gt; is the simplest way of controlling the smoothing parameter: the lower the value, the less smooth the curve (picks up on more variation). We can similarly adjust the histogram by increasing (decreasing) the number of &lt;code&gt;bins&lt;/code&gt;. There is a variety of methods to calculate kernel density and this is controlled through the &lt;code&gt;kernel&lt;/code&gt; parameter (“gaussian” by default).&lt;/p&gt;
&lt;p&gt;Although caution is required – we are after all estimating a smoothed curve from discrete data points – I have found kernel density estimation to be a valuable tool for exploring temporal patterns. In most cases it is decisively superior to a bar chart with arbitrarily grouped counts on the y-axis. The below is an example from my PhD thesis where I compare a theoretical model of time variation with a kernel density estimate drawn from submission dates of court cases.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/temporal.png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
