<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data | Michal Ovádek</title>
    <link>https://michalovadek.github.io/category/data/</link>
      <atom:link href="https://michalovadek.github.io/category/data/index.xml" rel="self" type="application/rss+xml" />
    <description>Data</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Michal Ovádek</copyright><lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://michalovadek.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Data</title>
      <link>https://michalovadek.github.io/category/data/</link>
    </image>
    
    <item>
      <title>Grid search for elastic net regularization</title>
      <link>https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/grid-search-for-elastic-net-regularization/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/grid-search-for-elastic-net-regularization/</guid>
      <description>&lt;p&gt;This post is a footnote to documentation to the &lt;code&gt;glmnet&lt;/code&gt; package and the &lt;code&gt;tidymodels&lt;/code&gt; framework. &lt;code&gt;glmnet&lt;/code&gt; is best known for fitting models via penalized maximum likelihood like ridge, lasso and elastic net regression. As explained in its documentatiom, &lt;code&gt;glmnet&lt;/code&gt; solves the problem&lt;/p&gt;
&lt;p&gt;$\begin{align}
\min_{\beta_0,\beta_1}\frac{1}{N}\sum_{i=1}^Nw_il(y_i,\beta_0+β^Tx_i)+\lambda[(1−\alpha)∥\beta∥^2_2/2+\alpha∥\beta∥_1]
\end{align}$&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;glmnet::glmnet()&lt;/code&gt; conducts a grid search over values of &lt;em&gt;$\lambda$&lt;/em&gt; which controls the overall strength of the penalty in the second term. When &lt;em&gt;$\alpha = 1$&lt;/em&gt; we speak of lasso regression which can shrink coefficients to zero (discard them), while ridge regression ($\alpha = 0$) does not remove features.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;glmnet::glmnet()&lt;/code&gt; expects the user to set the value of &lt;em&gt;$\alpha \in [0,1]$&lt;/em&gt;. In practice, however, many users will not know whether ridge, lasso or their mixture is most appropriate to their problem. Especially when we only care about minimizing &lt;em&gt;$|\hat{y} - y|$&lt;/em&gt; we might want to conduct a grid search over both &lt;em&gt;$\lambda$&lt;/em&gt; and &lt;em&gt;$\alpha$&lt;/em&gt; to find the most suitable tuple of hyperparameter values for our model. The example below shows that this dual search is particularly easy to implement with &lt;code&gt;tidymodels&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;house-prices-in-slovakia&#34;&gt;House prices in Slovakia&lt;/h2&gt;
&lt;p&gt;To exemplify the grid search I use a toy dataset with possible determinants of average quarterly house prices ($y$) in Slovakia. Let&amp;rsquo;s pretend that we have no idea about what affects house prices. Our design matrix &lt;em&gt;$X$&lt;/em&gt; consists of a bunch of more or less reasonable predictors like average wage and net migration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load the data
data &amp;lt;- &amp;quot;https://github.com/michalovadek/misc/blob/main/sk_house_prices.RData?raw=true&amp;quot;
load(url(data))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will need the &lt;code&gt;tidymodels&lt;/code&gt; set of packages and a seed for replicability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidymodels)
#require(glmnet)

set.seed(94241)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our goal is to find a penalized regression model that is good at predicting house prices with the available predictors. To mitigate overfitting, we will both split our data into a training and a test set and deploy k-fold cross-validation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# let&#39;s not worry about missing values and time ids
data &amp;lt;- dat_agg |&amp;gt; 
  dplyr::select(-year,-quarter) |&amp;gt; 
  tidyr::fill(dplyr::everything(), .direction = &amp;quot;updown&amp;quot;)

# we keep 1/4 of the observations for testing
split &amp;lt;- initial_split(data,
                       prop = 3/4)

train_data &amp;lt;- training(split)
test_data &amp;lt;- testing(split)

# k-fold cross-validation
folds &amp;lt;- vfold_cv(train_data, v = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We define the model and workflow for fitting it. We will be conducting a grid search over both &lt;code&gt;penalty&lt;/code&gt; ($\lambda$) and &lt;code&gt;mixture&lt;/code&gt; ($\alpha$).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define model
model &amp;lt;- linear_reg(penalty = tune(), mixture = tune()) |&amp;gt;
  set_engine(&amp;quot;glmnet&amp;quot;)

# RHS in formula selects all predictors
model_wf &amp;lt;- workflow() |&amp;gt;
  add_model(model) |&amp;gt;
  add_formula(price ~ .)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the grid search. We are looking for the best combination of &lt;em&gt;$\lambda$&lt;/em&gt; and &lt;em&gt;$\alpha$&lt;/em&gt;. We create this grid with the appropriately named &lt;code&gt;expand_grid()&lt;/code&gt; (or &lt;code&gt;expand.grid&lt;/code&gt; in &lt;code&gt;base&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# suitable values of lambda will vary
# you can run glmnet::glmnet() to get the ballpark range for your data
grid &amp;lt;- tidyr::expand_grid(penalty = seq(0.1, 15, length.out = 50),
                           mixture = c(0, 0.33, 0.66, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are ready to search the hyperparameter space using our k-folded training data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# we will use R^2 to evaluate model performance
search_res &amp;lt;- model_wf |&amp;gt; 
  tune_grid(folds,
            grid = grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rsq))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results can be parsed manually by running &lt;code&gt;collect_metrics()&lt;/code&gt; but &lt;code&gt;tidymodels&lt;/code&gt; comes with a handy &lt;code&gt;autoplot()&lt;/code&gt; method that will give us a quick answer as to which hyperparameter tuple performed best.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;autoplot(search_res)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/2021-12-05-grid-search-for-elastic-net-regularization_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can immediately see some clear differences between the model options. For the selected range of &lt;em&gt;$\lambda$&lt;/em&gt;, the performance of ridge regression does not change. Given how small our toy dataset is (and $N_{obs} &amp;gt; N_{predictors}$), it is not surprising that small penalties perform best.&lt;/p&gt;
&lt;p&gt;The workflow is completed by picking and fitting the best model and seeing how it performs on unseen data (our test set).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# select model by highest R^2
best_param &amp;lt;- search_res |&amp;gt; select_best()

# final workflow
model_wf_final &amp;lt;- finalize_workflow(model_wf, best_param)

# fit final model
final_fit &amp;lt;- model_wf_final |&amp;gt; 
  fit(data = train_data)

# predict using test set
test_data$.pred &amp;lt;- predict(final_fit, new_data = test_data)$.pred
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot y and yhat
test_data |&amp;gt; 
  ggplot(aes(x = price, y = .pred)) +
  geom_point() +
  geom_abline(slope = 1, color = &amp;quot;grey30&amp;quot;) +
  theme_minimal()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/2021-12-05-grid-search-for-elastic-net-regularization_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Looks good. Final performance metrics are obtained via &lt;code&gt;metrics()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;metrics(test_data, truth = price, estimate = .pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rmse    standard      72.5  
## 2 rsq     standard       0.938
## 3 mae     standard      47.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it. &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;tidymodels&lt;/code&gt; are a powerful and easy-to-use duo for high-dimensional data analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The eurlex R package</title>
      <link>https://michalovadek.github.io/post/2020-12-16-the-eurlex-r-package/the-eurlex-r-package/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2020-12-16-the-eurlex-r-package/the-eurlex-r-package/</guid>
      <description>&lt;p&gt;Dozens of political scientists and legal scholars use data on European Union laws in their research. The provenance of these data is rarely discussed. More often than not, researchers resort to the quick and dirty technique of scraping entire html pages from &lt;code&gt;eur-lex.europa.eu&lt;/code&gt;. This is not the optimal, nor preferred (from the perspective of the server host) approach of retrieving data, however, especially as the Publication Office of the European Union, the public body behind Eur-Lex, operates several dedicated APIs for automated retrieval of its data.&lt;/p&gt;
&lt;p&gt;The allure of web scraping is completely understandable. Not only is it easier to download data that can be readily seen in a user-friendly manner through a browser, using the dedicated APIs requires technical knowledge of semantic web and Client URL technologies, which is not necessarily widespread among researchers. And why go through the pain of learning how to compile SPARQL queries when it is much easier to simply download the web page?&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;eurlex&lt;/code&gt; &lt;a href=&#34;https://github.com/michalovadek/eurlex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R package&lt;/a&gt; attempts to significantly reduce the overhead associated with using the SPARQL and REST APIs made available by the EU Publication Office. Although at present it does not offer access to the same array of information as comprehensive web scraping might, the package provides simpler, more efficient and transparent access to data on European Union law. This vignette gives a quick guide to the package and an even quicker introduction to the Eur-Lex dataverse.&lt;/p&gt;
&lt;h1 id=&#34;the-eurlex-package&#34;&gt;The &lt;code&gt;eurlex&lt;/code&gt; package&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;eurlex&lt;/code&gt; package currently envisions the typical use-case to consist of getting bulk information about EU legislation into R as fast as possible. The package contains three core functions to achieve that objective: &lt;code&gt;elx_make_query()&lt;/code&gt; to create pre-defined or customized SPARQL queries; &lt;code&gt;elx_run_query()&lt;/code&gt; to execute the pre-made or any other manually input query; and &lt;code&gt;elx_fetch_data()&lt;/code&gt; to fire GET requests for certain metadata to the REST API.&lt;/p&gt;
&lt;h2 id=&#34;elx_make_query-generate-sparql-queries&#34;&gt;&lt;code&gt;elx_make_query()&lt;/code&gt;: Generate SPARQL queries&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;elx_make_query&lt;/code&gt; takes as its first argument the type of resource to be retrieved from the semantic database that powers Eur-Lex (and other publications) called Cellar.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(eurlex)
library(dplyr) # my preference, not needed for the package

query_dir &amp;lt;- elx_make_query(resource_type = &amp;quot;directive&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently, it is possible to choose from among a host of resource types, including directives, regulations and even case law (see function description for the full list). It is also possible to manually specify a resource type from the &lt;a href=&#34;http://publications.europa.eu/resource/authority/resource-type&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eligible list&lt;/a&gt;.^[Note, however, that not all resource types will work properly with the pre-specified query.]&lt;/p&gt;
&lt;p&gt;The choice of resource type is then reflected in the SPARQL query generated by the function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;query_dir %&amp;gt;% 
  glue::as_glue() # for nicer printing
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PREFIX cdm: &amp;lt;http://publications.europa.eu/ontology/cdm#&amp;gt;
##   PREFIX annot: &amp;lt;http://publications.europa.eu/ontology/annotation#&amp;gt;
##   PREFIX skos:&amp;lt;http://www.w3.org/2004/02/skos/core#&amp;gt;
##   PREFIX dc:&amp;lt;http://purl.org/dc/elements/1.1/&amp;gt;
##   PREFIX xsd:&amp;lt;http://www.w3.org/2001/XMLSchema#&amp;gt;
##   PREFIX rdf:&amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt;
##   PREFIX owl:&amp;lt;http://www.w3.org/2002/07/owl#&amp;gt;
##   select distinct ?work ?type ?celex where{ ?work cdm:work_has_resource-type ?type. FILTER(?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/DIR&amp;gt;||
##   ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/DIR_IMPL&amp;gt;||
##   ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/DIR_DEL&amp;gt;) 
##  FILTER not exists{?work cdm:work_has_resource-type &amp;lt;http://publications.europa.eu/resource/authority/resource-type/CORRIGENDUM&amp;gt;} OPTIONAL{?work cdm:resource_legal_id_celex ?celex.} }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are various ways of querying the same information in the Cellar database due to the existence of several overlapping classes and identifiers describing the same resources. The queries generated by the function should offer a reliable way of obtaining exhaustive results, as they have been validated by the helpdesk of the Publication Office. At the same time, it is always possible there will be issues either on the query or the database side; please report any you encounter through Github.&lt;/p&gt;
&lt;p&gt;The other arguments in &lt;code&gt;elx_make_query()&lt;/code&gt; relate to additional metadata to be returned. The results include by default the &lt;a href=&#34;https://eur-lex.europa.eu/content/tools/TableOfSectors/types_of_documents_in_eurlex.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CELEX number&lt;/a&gt; and exclude corrigenda (corrections of errors in legislation). Other data needs to be opted into. Make sure to select ones that are logically compatible (e.g. case law does not have a legal basis). More options should be added in the future.&lt;/p&gt;
&lt;p&gt;Note that availability of data for each variable has an impact on the results. The data frame returned by the query will be shrunken to the size of the variable with most missing data. It is recommended to always compare results from a desired query to a minimal query requesting only celex ids.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;elx_make_query(resource_type = &amp;quot;recommendation&amp;quot;, include_date = TRUE, include_lbs = TRUE) %&amp;gt;% 
  glue::as_glue()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PREFIX cdm: &amp;lt;http://publications.europa.eu/ontology/cdm#&amp;gt;
##   PREFIX annot: &amp;lt;http://publications.europa.eu/ontology/annotation#&amp;gt;
##   PREFIX skos:&amp;lt;http://www.w3.org/2004/02/skos/core#&amp;gt;
##   PREFIX dc:&amp;lt;http://purl.org/dc/elements/1.1/&amp;gt;
##   PREFIX xsd:&amp;lt;http://www.w3.org/2001/XMLSchema#&amp;gt;
##   PREFIX rdf:&amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt;
##   PREFIX owl:&amp;lt;http://www.w3.org/2002/07/owl#&amp;gt;
##   select distinct ?work ?type ?celex str(?date) ?lbs ?lbcelex ?lbsuffix where{ ?work cdm:work_has_resource-type ?type. FILTER(?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_DEC&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_DIR&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_OPIN&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_RES&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_REG&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_RECO&amp;gt;||
##                    ?type=&amp;lt;http://publications.europa.eu/resource/authority/resource-type/RECO_DRAFT&amp;gt;) 
##  FILTER not exists{?work cdm:work_has_resource-type &amp;lt;http://publications.europa.eu/resource/authority/resource-type/CORRIGENDUM&amp;gt;} OPTIONAL{?work cdm:resource_legal_id_celex ?celex.} OPTIONAL{?work cdm:work_date_document ?date.} OPTIONAL{?work cdm:resource_legal_based_on_resource_legal ?lbs.
##     ?lbs cdm:resource_legal_id_celex ?lbcelex.
##     OPTIONAL{?bn owl:annotatedSource ?work.
##     ?bn owl:annotatedProperty &amp;lt;http://publications.europa.eu/ontology/cdm#resource_legal_based_on_resource_legal&amp;gt;.
##     ?bn owl:annotatedTarget ?lbs.
##     ?bn annot:comment_on_legal_basis ?lbsuffix}} }
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# minimal query: elx_make_query(resource_type = &amp;quot;recommendation&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have a query, we are ready to run it.&lt;/p&gt;
&lt;h2 id=&#34;elx_run_query-execute-sparql-queries&#34;&gt;&lt;code&gt;elx_run_query()&lt;/code&gt;: Execute SPARQL queries&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;elx_run_query()&lt;/code&gt; sends SPARQL queries to a pre-specified endpoint. The function takes the query string as the main argument, which means you can manually pass it any working SPARQL query (relevant to official EU publications).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;results &amp;lt;- elx_run_query(query = query_dir)

# the functions are compatible with piping
# 
# elx_make_query(&amp;quot;directive&amp;quot;) %&amp;gt;% 
#   elx_run_query()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as_tibble(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,352 x 3
##    work                                  type                            celex  
##    &amp;lt;chr&amp;gt;                                 &amp;lt;chr&amp;gt;                           &amp;lt;chr&amp;gt;  
##  1 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31979L~
##  2 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31989L~
##  3 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31984L~
##  4 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31966L~
##  5 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31993L~
##  6 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31992L~
##  7 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31983L~
##  8 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31966L~
##  9 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31974L~
## 10 http://publications.europa.eu/resour~ http://publications.europa.eu/~ 31982L~
## # ... with 4,342 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function outputs a &lt;code&gt;data.frame&lt;/code&gt; where each column corresponds to one of the requested variables, while the rows accumulate observations of the resource type satisfying the query criteria. Obviously, the more data is to be returned, the longer the execution time, varying from a few seconds to several minutes, depending also on your connection.&lt;/p&gt;
&lt;p&gt;The first column always contains the unique URI of a &amp;ldquo;work&amp;rdquo; (legislative act or court judgment) which identifies each resource in Cellar. Several human-readable identifiers are normally associated with each &amp;ldquo;work&amp;rdquo; but the most useful one is CELEX, retrieved by default.^[Occasionally, you may encounter legal acts without CELEX numbers, especially when digging through older legislation. It is good to report these to the Eur-Lex helpdesk.]&lt;/p&gt;
&lt;p&gt;One column you should always pay attention to is &lt;code&gt;type&lt;/code&gt; (as in &lt;code&gt;resource_type&lt;/code&gt;). The URIs contained there reflect the FILTER argument in the SPARQL query, which is manually pre-specified. All resources are indexed as being of one type or another. For example, when retrieving directives, the results are going to return also delegated directives, which might not be desirable, depending on your needs. You can filter results by &lt;code&gt;type&lt;/code&gt; to make the necessary adjustments. The queries are expansive by default in the spirit of erring on the side of over-inclusiveness rather than vice versa.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(results$type,5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://publications.europa.eu/resource/authority/resource-type/DIR&amp;quot;
## [2] &amp;quot;http://publications.europa.eu/resource/authority/resource-type/DIR&amp;quot;
## [3] &amp;quot;http://publications.europa.eu/resource/authority/resource-type/DIR&amp;quot;
## [4] &amp;quot;http://publications.europa.eu/resource/authority/resource-type/DIR&amp;quot;
## [5] &amp;quot;http://publications.europa.eu/resource/authority/resource-type/DIR&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;results %&amp;gt;% 
  distinct(type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 1
##   type                                                                   
##   &amp;lt;chr&amp;gt;                                                                  
## 1 http://publications.europa.eu/resource/authority/resource-type/DIR     
## 2 http://publications.europa.eu/resource/authority/resource-type/DIR_IMPL
## 3 http://publications.europa.eu/resource/authority/resource-type/DIR_DEL
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is returned in the long format, which means that rows are recycled up to the length of the variable with the most data points. For example, if 20 directives are returned, each with two legal bases, the resulting &lt;code&gt;data.frame&lt;/code&gt; will have 40 rows. Some variables, such as dates, contain unexpectedly several entries for some documents. You should always check the number of unique identifiers in the results instead of assuming that each row is a unique observation.&lt;/p&gt;
&lt;h3 id=&#34;eurovoc-descriptors&#34;&gt;EuroVoc descriptors&lt;/h3&gt;
&lt;p&gt;EuroVoc is a multilingual thesaurus, keywords from which are used to describe the content of European Union documents. Most resource types that can be retrieved with the pre-defined queries in this package can be accompanied by EuroVoc keywords and these can be retrieved as other variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rec_eurovoc &amp;lt;- elx_make_query(&amp;quot;recommendation&amp;quot;, include_eurovoc = TRUE, limit = 10) %&amp;gt;% 
  elx_run_query() # truncated results for sake of the example

rec_eurovoc %&amp;gt;% 
  select(celex, eurovoc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    celex          eurovoc                      
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                        
##  1 32012H0090     http://eurovoc.europa.eu/1425
##  2 31962H0816     http://eurovoc.europa.eu/1004
##  3 31974H0435     http://eurovoc.europa.eu/1085
##  4 31996H0592     http://eurovoc.europa.eu/1076
##  5 32015H0818(10) http://eurovoc.europa.eu/1021
##  6 31974H0256     http://eurovoc.europa.eu/1318
##  7 31999H0333     http://eurovoc.europa.eu/1442
##  8 52004SC0086    http://eurovoc.europa.eu/1504
##  9 32003H0579     http://eurovoc.europa.eu/114 
## 10 E2004C0055     http://eurovoc.europa.eu/1442
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the endpoint returns the EuroVoc concept codes rather than the labels (keywords). The function &lt;code&gt;elx_label_eurovoc()&lt;/code&gt; needs to be called to obtain a look-up table with the labels.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;eurovoc_lookup &amp;lt;- elx_label_eurovoc(uri_eurovoc = rec_eurovoc$eurovoc)

print(eurovoc_lookup)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
##   eurovoc                       labels                       
##   &amp;lt;chr&amp;gt;                         &amp;lt;chr&amp;gt;                        
## 1 http://eurovoc.europa.eu/1085 France                       
## 2 http://eurovoc.europa.eu/1442 food inspection              
## 3 http://eurovoc.europa.eu/1076 form                         
## 4 http://eurovoc.europa.eu/1318 Germany                      
## 5 http://eurovoc.europa.eu/1504 Ireland                      
## 6 http://eurovoc.europa.eu/114  Council of the European Union
## 7 http://eurovoc.europa.eu/1425 consumer information         
## 8 http://eurovoc.europa.eu/1004 welfare                      
## 9 http://eurovoc.europa.eu/1021 tax system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results include labels only for unique identifiers, but with &lt;code&gt;dplyr::left_join()&lt;/code&gt; it is straightforward to append the labels to the entire dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rec_eurovoc %&amp;gt;% 
  left_join(eurovoc_lookup)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;eurovoc&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 5
##    work                    type                 celex   eurovoc      labels     
##    &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;      
##  1 http://publications.eu~ http://publications~ 32012H~ http://euro~ consumer i~
##  2 http://publications.eu~ http://publications~ 31962H~ http://euro~ welfare    
##  3 http://publications.eu~ http://publications~ 31974H~ http://euro~ France     
##  4 http://publications.eu~ http://publications~ 31996H~ http://euro~ form       
##  5 http://publications.eu~ http://publications~ 32015H~ http://euro~ tax system 
##  6 http://publications.eu~ http://publications~ 31974H~ http://euro~ Germany    
##  7 http://publications.eu~ http://publications~ 31999H~ http://euro~ food inspe~
##  8 http://publications.eu~ http://publications~ 52004S~ http://euro~ Ireland    
##  9 http://publications.eu~ http://publications~ 32003H~ http://euro~ Council of~
## 10 http://publications.eu~ http://publications~ E2004C~ http://euro~ food inspe~
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As elsewhere in the API, we can tap into the multilingual nature of EU documents also when it comes to the EuroVoc keywords. Moreover, most concepts in the thesaurus are associated with alternative labels; these can be returned as well (separated by a comma).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;eurovoc_lookup &amp;lt;- elx_label_eurovoc(uri_eurovoc = rec_eurovoc$eurovoc,
                                    alt_labels = TRUE,
                                    language = &amp;quot;sk&amp;quot;)

rec_eurovoc %&amp;gt;% 
  left_join(eurovoc_lookup) %&amp;gt;% 
  select(celex, eurovoc, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;eurovoc&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    celex          eurovoc                       labels                          
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                         &amp;lt;chr&amp;gt;                           
##  1 32012H0090     http://eurovoc.europa.eu/1425 informácie pre spotrebitela,vzd~
##  2 31962H0816     http://eurovoc.europa.eu/1004 blahobyt                        
##  3 31974H0435     http://eurovoc.europa.eu/1085 Francúzska republika,Francúzsko 
##  4 31996H0592     http://eurovoc.europa.eu/1076 formulár                        
##  5 32015H0818(10) http://eurovoc.europa.eu/1021 danová sústava,danový systém    
##  6 31974H0256     http://eurovoc.europa.eu/1318 NSR,Nemecko,Nemecká spolková re~
##  7 31999H0333     http://eurovoc.europa.eu/1442 analýza potravín,kontrola potra~
##  8 52004SC0086    http://eurovoc.europa.eu/1504 južné Írsko,Írsko               
##  9 32003H0579     http://eurovoc.europa.eu/114  Rada Európskej únie,Rada Európs~
## 10 E2004C0055     http://eurovoc.europa.eu/1442 analýza potravín,kontrola potra~
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;elx_fetch_data-fire-get-requests&#34;&gt;&lt;code&gt;elx_fetch_data()&lt;/code&gt;: Fire GET requests&lt;/h2&gt;
&lt;p&gt;A core contribution of the SPARQL requests is that we obtain a comprehensive list of identifiers that we can subsequently use to obtain more data relating to the document in question. While the results of the SPARQL queries are useful also for webscraping (with the &lt;code&gt;rvest&lt;/code&gt; package), the function &lt;code&gt;elx_fetch_data()&lt;/code&gt; enables us to fire GET requests to retrieve data on documents with known identifiers (including Cellar URI).&lt;/p&gt;
&lt;p&gt;One of the most sought-after data in the Eur-Lex dataverse is the text. It is possible now to automate the pipeline for downloading html and plain texts from Eur-Lex. Similarly, you can retrieve the title of the document. For both you can specify also the desired language (English by default). Other metadata might be added in the future.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# the function is not vectorized by default
elx_fetch_data(results$work[1],&amp;quot;title&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Council Directive 79/173/EEC of 6 February 1979 on the programme for the acceleration and guidance of collective irrigation works in Corsica&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# we can use purrr::map() to play that role
library(purrr)

dir_titles &amp;lt;- results[1:10,] %&amp;gt;% # take the first 10 directives only to save time
  mutate(title = map_chr(work,elx_fetch_data, &amp;quot;title&amp;quot;)) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  select(celex, title)

print(dir_titles)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    celex      title                                                             
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                                                             
##  1 31979L0173 Council Directive 79/173/EEC of 6 February 1979 on the programme ~
##  2 31989L0194 Council Directive 89/194/EEC of 13 March 1989 amending Directive ~
##  3 31984L0378 Council Directive 84/378/EEC of 28 June 1984 amending the Annexes~
##  4 31966L0683 Commission Directive 66/683/EEC of 7 November 1966 eliminating al~
##  5 31993L0004 Council Directive 93/4/EEC of 8 February 1993 amending Directive ~
##  6 31992L0017 Commission Directive 92/17/EEC of 17 March 1992 amending Directiv~
##  7 31983L0447 Commission Directive 83/447/EEC of 18 August 1983 adopting the me~
##  8 31966L0162 Council Directive 66/162/EEC of 28 February 1966 concerning the a~
##  9 31974L0508 Commission Directive 74/508/EEC of 30 September 1974 amending Com~
## 10 31982L0957 Third Commission Directive 82/957/EEC of 22 December 1982 amendin~
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that text requests are by far the most time-intensive; requesting the full text for thousands of documents is liable to extend the run-time into hours. Texts are retrieved from html by priority, but methods for pdfs and .docs are also implemented.^[It is worth pointing out that the html and pdf contents of older case law differs. Whereas typically the html file is only going to contain a summary and grounds of a judgment, the pdf should also contain background to the dispute.] The function even handles multi-document resources (by pasting them together).&lt;/p&gt;
&lt;h1 id=&#34;application&#34;&gt;Application&lt;/h1&gt;
&lt;p&gt;In this section I showcase a simple application of &lt;code&gt;eurlex&lt;/code&gt; on making overviews of EU legislation. First, we collate data on directives.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dirs &amp;lt;- elx_make_query(resource_type = &amp;quot;directive&amp;quot;, include_date = TRUE, include_force = TRUE) %&amp;gt;% 
  elx_run_query() %&amp;gt;% 
  rename(date = `callret-3`)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s calculate the proportion of directives currently in force in the entire set of directives ever adopted. This variable offers a particularly good demonstration of the usefulness of the package to retrieve EU law data, because it changes every day, as new acts enter into force and old ones drop out. Regularly scraping webpages for this purpose and scale is simply impractical and disproportional.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)

dirs %&amp;gt;% 
  count(force) %&amp;gt;% 
  ggplot(aes(x = force, y = n)) +
  geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2020-12-16-the-eurlex-r-package/2020-12-16-the-eurlex-r-package_files/figure-html/firstplot-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Directives become naturally outdated with time. It might be all the more interesting to see which older acts are thus still surviving.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dirs %&amp;gt;% 
  ggplot(aes(x = as.Date(date), y = celex)) +
  geom_point(aes(color = force), alpha = 0.1) +
  theme(axis.text.y = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2020-12-16-the-eurlex-r-package/2020-12-16-the-eurlex-r-package_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We want to know a bit more about the directives from 1970s that are still in force today. Their titles could give us a clue.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dirs_1970_title &amp;lt;- dirs %&amp;gt;% 
  filter(between(as.Date(date), as.Date(&amp;quot;1970-01-01&amp;quot;), as.Date(&amp;quot;1980-01-01&amp;quot;)),
         force == &amp;quot;true&amp;quot;) %&amp;gt;% 
  mutate(title = map_chr(work,elx_fetch_data,&amp;quot;title&amp;quot;)) %&amp;gt;% 
  as_tibble()

print(dirs_1970_title)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 6
##    work                 type              celex  date  force title              
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;              
##  1 http://publications~ http://publicati~ 31975~ 1975~ true  Council Directive ~
##  2 http://publications~ http://publicati~ 31977~ 1977~ true  First Commission D~
##  3 http://publications~ http://publicati~ 31977~ 1977~ true  Council Directive ~
##  4 http://publications~ http://publicati~ 31973~ 1973~ true  Council Directive ~
##  5 http://publications~ http://publicati~ 31974~ 1974~ true  Council Directive ~
##  6 http://publications~ http://publicati~ 31972~ 1972~ true  Council Directive ~
##  7 http://publications~ http://publicati~ 31972~ 1972~ true  Council Directive ~
##  8 http://publications~ http://publicati~ 31977~ 1977~ true  Council Directive ~
##  9 http://publications~ http://publicati~ 31979~ 1978~ true  Council Directive ~
## 10 http://publications~ http://publicati~ 31979~ 1979~ true  Council Directive ~
## # ... with 54 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will use the &lt;code&gt;tidytext&lt;/code&gt; package to get a quick idea of what the legislation is about.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidytext)
library(wordcloud)

dirs_1970_title %&amp;gt;% 
  select(celex,title) %&amp;gt;% 
  unnest_tokens(word, title) %&amp;gt;% 
  count(celex, word, sort = TRUE) %&amp;gt;% 
  filter(!grepl(&amp;quot;\\d&amp;quot;, word)) %&amp;gt;% 
  bind_tf_idf(word, celex, n) %&amp;gt;% 
  with(wordcloud(word, tf_idf, max.words = 40, scale = c(1.8,0.1)))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2020-12-16-the-eurlex-r-package/2020-12-16-the-eurlex-r-package_files/figure-html/wordcloud-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;I use term-frequency inverse-document frequency (tf-idf) to weight the importance of the words in the wordcloud. If we used pure frequencies, the wordcloud would largely consist of words conveying little meaning (&amp;ldquo;the&amp;rdquo;, &amp;ldquo;and&amp;rdquo;, &amp;hellip;).&lt;/p&gt;
&lt;p&gt;This is an extremely basic application of the &lt;code&gt;eurlex&lt;/code&gt; package. Much more sophisticated methods can be used to analyse both the content and metadata of European Union legislation. If the package is useful for your research, please consider citing it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernel density estimation for time series plots</title>
      <link>https://michalovadek.github.io/post/2019-12-07-time-series-data-and-kernel-density/time-series-data-and-kernel-density/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2019-12-07-time-series-data-and-kernel-density/time-series-data-and-kernel-density/</guid>
      <description>&lt;p&gt;In my research I frequently encounter count data that has a temporal dimension. The standard way of visualizing such data is to take the counts grouped at some temporal level (month, year, etc). The clustering essentially does away with the problem of the y-axis, namely in that it creates variation at evenly spaced out x-axis intervals. Let&amp;rsquo;s generate randomly a bunch of dates and show an archetypal bar plot at the year level of, say, judgments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(lubridate)

set.seed(92139)

dates &amp;lt;- as_date(as.Date(&amp;quot;2000-01-01&amp;quot;):as.Date(&amp;quot;2020-01-01&amp;quot;))

dat &amp;lt;- sample(dates, 300, replace = F,
              prob = abs(rnorm(length(dates),100, 200))
              )

dat %&amp;gt;% 
  enframe() %&amp;gt;% 
  mutate(year = as.integer(str_sub(value,1,4))) %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  count() %&amp;gt;% 
  ggplot(aes(x = year, y = n)) +
  geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2019-12-07-time-series-data-and-kernel-density/2019-12-07-time-series-data-and-kernel-density_files/figure-html/barplot-1.png&#34; alt=&#34;Number of fictitious judgments between 2000-2020&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1: Number of fictitious judgments between 2000-2020&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Nothing very special here. However, note that instead of using the actual &lt;em&gt;dates&lt;/em&gt; of the judgments, we sacrifice precision to obtain variation on the y-axis. Often the grouping (temporal) level &amp;ndash; year in our case &amp;ndash; does not carry any particular meaning beyond the fact that years are readily comprehensible to everyone. Can we do better?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the same data and visualize the date of each judgment. Without any additional variable that could supply variation along a second dimension, I simply fix &lt;em&gt;y = 1&lt;/em&gt; to denote that each judgment is counted, well, once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat %&amp;gt;% 
  enframe() %&amp;gt;% 
  ggplot(aes(x = value, y = 1)) +
  geom_point(alpha = 0.1) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2019-12-07-time-series-data-and-kernel-density/2019-12-07-time-series-data-and-kernel-density_files/figure-html/dots-1.png&#34; alt=&#34;Dates of fictitious judgments between 2000-2020&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 2: Dates of fictitious judgments between 2000-2020&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The visualization is not very good. Although the x-axis is now a proper continuous timeseries measured in days, there are too many judgments and days in the 20-year period to see clearly any patterns in the data. Increasing the transparency of the data points is a good first step, as darker areas indicate greater &lt;em&gt;density&lt;/em&gt; of judgments around that time.&lt;/p&gt;
&lt;p&gt;We can exploit the full potential of the dates using the statistical concept of &lt;em&gt;kernel density&lt;/em&gt;. For ease of interpretation, we can think of this as a smoothed histogram. It is very straightforward to implement in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat %&amp;gt;% 
  enframe() %&amp;gt;% 
  ggplot(aes(x = value)) +
  geom_vline(xintercept = as.Date(&amp;quot;2013-01-01&amp;quot;), lty = 2) +
  annotate(&amp;quot;text&amp;quot;, x = as.Date(&amp;quot;2014-12-31&amp;quot;), y = 0.99, label = &amp;quot;Some event&amp;quot;, fontface = &amp;quot;bold&amp;quot;) +
  geom_histogram(alpha = 0.1, aes(y = ..ncount..), bins = 40) +
  stat_density(geom = &amp;quot;line&amp;quot;, adjust = 1/10, kernel = &amp;quot;gaussian&amp;quot;, aes(y = ..scaled..)) +
  geom_point(alpha = 0.1, y = 0.1) +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  labs(x = NULL, y = &amp;quot;Kernel density estimates&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2019-12-07-time-series-data-and-kernel-density/2019-12-07-time-series-data-and-kernel-density_files/figure-html/kde-1.png&#34; alt=&#34;Dates of fictitious judgments between 2000-2020 with density estimated&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 3: Dates of fictitious judgments between 2000-2020 with density estimated&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using &lt;code&gt;stat_density&lt;/code&gt; we get a nice continuous (estimated) timeseries of our data. The idea is to basically treat the time dimension as just a series of numbers over which a continuous distribution of our data can be drawn.&lt;/p&gt;
&lt;p&gt;Setting &lt;code&gt;y = ..scaled..&lt;/code&gt; scales density to the 0-1 range, which is useful if we want to, for example, compare the kernel density estimates with a histogram that we similarly scale with &lt;code&gt;y = ..ncount..&lt;/code&gt;. Changing &lt;code&gt;adjust&lt;/code&gt; is the simplest way of controlling the smoothing parameter: the lower the value, the less smooth the curve (picks up on more variation). We can similarly adjust the histogram by increasing (decreasing) the number of &lt;code&gt;bins&lt;/code&gt;. There is a variety of methods to calculate kernel density and this is controlled through the &lt;code&gt;kernel&lt;/code&gt; parameter (&amp;ldquo;gaussian&amp;rdquo; by default).&lt;/p&gt;
&lt;p&gt;Although caution is required &amp;ndash; we are after all estimating a smoothed curve from discrete data points &amp;ndash; I have found kernel density estimation to be a valuable tool for exploring temporal patterns. In most cases it is decisively superior to a bar chart with arbitrarily grouped counts on the y-axis. The below is an example from my PhD thesis where I compare a theoretical model of time variation with a kernel density estimate drawn from submission dates of court cases.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34; &#34; srcset=&#34;
               /media/temporal_hu30d20394123abe532c5f7fba28c9db35_54877_0286f5320d1d72c0a7d7c4f9c5d81c96.png 400w,
               /media/temporal_hu30d20394123abe532c5f7fba28c9db35_54877_564d21158a67efb3417ba0573ea2df85.png 760w,
               /media/temporal_hu30d20394123abe532c5f7fba28c9db35_54877_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://michalovadek.github.io/media/temporal_hu30d20394123abe532c5f7fba28c9db35_54877_0286f5320d1d72c0a7d7c4f9c5d81c96.png&#34;
               width=&#34;760&#34;
               height=&#34;738&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pre-processing steps in quantitative text analysis</title>
      <link>https://michalovadek.github.io/post/2019-07-08-pre-processing-steps-in-quantitative-text-analysis/pre-processing-steps-in-quantitative-text-analysis/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2019-07-08-pre-processing-steps-in-quantitative-text-analysis/pre-processing-steps-in-quantitative-text-analysis/</guid>
      <description>&lt;p&gt;In my work I often resort to topic modelling to summarize otherwise intractable textual corpora. Anyone even vaguely familiar with topic modelling and quantitative text methods more generally will acknowledge that how the raw text is prepared in the run-up to analysis is of critical importance. In a &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3393734&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt; with my co-authors we tried to document in a simple graph the various steps taken before running a structural topic model. (Interactive plots relating to the paper &lt;a href=&#34;https://euthority.eu/?page_id=660&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Although not all text-data requires this much attention, the graph illustrates the range of possibilities when it comes to pre-processing texts. Credit to Nicolas Lampach for the implementation with &lt;a href=&#34;https://rich-iannone.github.io/DiagrammeR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiagrammeR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Text pre-processing steps&#34; srcset=&#34;
               /media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_631e935804b86fa0afb0b43a80ff3bed.JPG 400w,
               /media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_329d01303bd5f1723a4a4bc9c8693837.JPG 760w,
               /media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://michalovadek.github.io/media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_631e935804b86fa0afb0b43a80ff3bed.JPG&#34;
               width=&#34;636&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;All of the pre-processing was done in R with the following packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quality images: &lt;code&gt;magick&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;OCR: &lt;code&gt;tesseract&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Tokenization and data reduction: &lt;code&gt;quanteda&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Language detection: &lt;code&gt;cld2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Spell-checking: &lt;code&gt;hunspell&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Simulating p-values in a 2x2 design</title>
      <link>https://michalovadek.github.io/post/2019-07-05-simulating-p-values-in-a-2x2-design/simulating-p-values-in-a-2x2-design/</link>
      <pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2019-07-05-simulating-p-values-in-a-2x2-design/simulating-p-values-in-a-2x2-design/</guid>
      <description>&lt;p&gt;In a recent &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3352467&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; in which I analysed experimental data on law students&#39; aversion to politically motivated legal arguments, I realized there was a fairly trivial but systematic relationship between changes in the distribution of outcomes drawn from a binomial distribution in the control group and p-values. For the graphically minded among us (myself) a picture is worth a thousand Greek letters, so below I am including a plot explaining the relationship on simulated data, including the simulation and &lt;code&gt;ggplot2&lt;/code&gt; code. The data concerns a 2x2 contingency matrix (control, treatment x 2 conditions with varying impact on outcome distribution in the control group) and p-values are obtained using Fisher&amp;rsquo;s exact test for count data (one-sided).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(Exact)
library(ggrepel)

# simulate

set.seed(1923)

n = as.integer(40)
amb &amp;lt;- seq(0.05,0.50,by=0.05) 
  
d &amp;lt;- dbinom(x = 0:n, size = n, prob = amb[1]) %&amp;gt;% 
  tibble(x = 0:n, n, p = ., amb = amb[1])

for (i in 1:length(amb)){
  
  d &amp;lt;- dbinom(x = 0:n, size = n, prob = amb[i]) %&amp;gt;% 
    tibble(x = 0:n, n, p = ., amb = amb[i]) %&amp;gt;% 
    bind_rows(d,.)
  
}

sampled &amp;lt;- d %&amp;gt;% 
  sample_n(10, replace = TRUE, weight = p) %&amp;gt;% # increase sample to &amp;gt; 1000
  rename(b = x) %&amp;gt;% 
  mutate(a = as.integer(n - b),
         amb_obs = b/n) %&amp;gt;% 
  crossing(c = 0:40) %&amp;gt;% 
  mutate(d = as.integer(n - c)) %&amp;gt;% 
  rowwise() %&amp;gt;% 
  mutate(p.value = fisher.test(tibble(j = c(a,c), k = c(b,d)),alternative = &amp;quot;greater&amp;quot;) %&amp;gt;% 
           broom::tidy() %&amp;gt;% 
           select(p.value) %&amp;gt;% 
           deframe())

dat &amp;lt;- sampled %&amp;gt;% 
  group_by(amb,d) %&amp;gt;% 
  summarise(p.value = mean(p.value)) %&amp;gt;% 
  mutate(lbl = ifelse(amb %in% c(0.05,0.50) &amp;amp; p.value &amp;lt; 0.05 &amp;amp; p.value &amp;gt; 0.035,paste(&amp;quot;x = &amp;quot;,d,sep = &amp;quot;&amp;quot;),NA))

# plot

dat %&amp;gt;% 
  ggplot(aes(x=d, y = p.value, color = amb)) +
  geom_hline(yintercept = 0.05, color = &amp;quot;grey90&amp;quot;, lty = 2, size = 1.2) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 35, y = 0.1, label = expression(alpha~&amp;quot; = 0.05&amp;quot;), color = &amp;quot;grey90&amp;quot;, size = 5) +
  geom_point(alpha = 0.7) +
  #geom_smooth(aes(y = p.value, color = amb)) +
  geom_label_repel(aes(label = lbl, x = d, y = p.value),
                   fontface = &amp;quot;italic&amp;quot;,
                   segment.color = &amp;quot;grey80&amp;quot;,
                   segment.alpha = 0.5,
                   segment.size = 0.8,
                   nudge_x = ifelse(deframe(na.omit(dat)[,2])&amp;gt;32,2,-3.7),
                   nudge_y = ifelse(deframe(na.omit(dat)[,2])&amp;gt;32,0.1,-0.03)) +
  scale_color_distiller(palette = &amp;quot;Spectral&amp;quot;, direction = -1,
                        limits = c(0.05,0.50),
                        breaks = c(0.05,0.20,0.35,0.50),
                        labels = c(0.05,0.20,0.35,0.50)) +
  theme_minimal() +
  labs(x = &amp;quot;Number of hypothesis-confirming responses in treatment group&amp;quot;, y = &amp;quot;p-value&amp;quot;, color = &amp;quot;Ambiguity&amp;quot;) +
  theme(panel.background = element_rect(fill = &amp;quot;grey20&amp;quot;),
        panel.grid = element_line(color = &amp;quot;grey40&amp;quot;),
        legend.position = c(0.775,0.72),
        legend.background = element_blank(),
        legend.key.size = unit(10,&amp;quot;mm&amp;quot;),
        legend.text = element_text(color = &amp;quot;grey90&amp;quot;),
        legend.title = element_text(color = &amp;quot;grey90&amp;quot;, face = &amp;quot;bold&amp;quot;),
        legend.spacing.y = unit(4,&amp;quot;mm&amp;quot;),
        legend.box = &amp;quot;horizontal&amp;quot;,
        legend.direction = &amp;quot;horizontal&amp;quot;,
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_line(color = &amp;quot;grey30&amp;quot;, linetype = 2),
        panel.grid.major.x = element_line(linetype = 3),
        panel.grid.minor.x = element_blank()) +
  guides(color = guide_colourbar(title.position=&amp;quot;top&amp;quot;, title.hjust = 0.5, reverse = F))
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2019-07-05-simulating-p-values-in-a-2x2-design/2019-07-05-simulating-p-values-in-a-2x2-design_files/figure-html/unnamed-chunk-1-1.png&#34; alt=&#34;Note the low number of samples makes some lines look crooked. In a real application the number of iterations must be at least 1000. I am skimping on samples here to reduce computation.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1: Note the low number of samples makes some lines look crooked. In a real application the number of iterations must be at least 1000. I am skimping on samples here to reduce computation.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This little project also made me realize that the spectral color theme looks rather good on a dark background. Although not the most conventional, I will definitely consider darker plot backgrounds in future visualizations.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
