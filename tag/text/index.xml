<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>text | Michal Ovádek</title>
    <link>https://michalovadek.github.io/tag/text/</link>
      <atom:link href="https://michalovadek.github.io/tag/text/index.xml" rel="self" type="application/rss+xml" />
    <description>text</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Michal Ovádek</copyright><lastBuildDate>Mon, 08 Jul 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://michalovadek.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>text</title>
      <link>https://michalovadek.github.io/tag/text/</link>
    </image>
    
    <item>
      <title>Pre-processing steps in quantitative text analysis</title>
      <link>https://michalovadek.github.io/post/2019-07-08-pre-processing-steps-in-quantitative-text-analysis/pre-processing-steps-in-quantitative-text-analysis/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2019-07-08-pre-processing-steps-in-quantitative-text-analysis/pre-processing-steps-in-quantitative-text-analysis/</guid>
      <description>&lt;p&gt;In my work I often resort to topic modelling to summarize otherwise intractable textual corpora. Anyone even vaguely familiar with topic modelling and quantitative text methods more generally will acknowledge that how the raw text is prepared in the run-up to analysis is of critical importance. In a &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3393734&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt; with my co-authors we tried to document in a simple graph the various steps taken before running a structural topic model. (Interactive plots relating to the paper &lt;a href=&#34;https://euthority.eu/?page_id=660&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Although not all text-data requires this much attention, the graph illustrates the range of possibilities when it comes to pre-processing texts. Credit to Nicolas Lampach for the implementation with &lt;a href=&#34;https://rich-iannone.github.io/DiagrammeR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiagrammeR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Text pre-processing steps&#34; srcset=&#34;
               /media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_631e935804b86fa0afb0b43a80ff3bed.JPG 400w,
               /media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_329d01303bd5f1723a4a4bc9c8693837.JPG 760w,
               /media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://michalovadek.github.io/media/preprocessing_hue1adbf098e97e38c9916862dd76887b6_75565_631e935804b86fa0afb0b43a80ff3bed.JPG&#34;
               width=&#34;636&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;All of the pre-processing was done in R with the following packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quality images: &lt;code&gt;magick&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;OCR: &lt;code&gt;tesseract&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Tokenization and data reduction: &lt;code&gt;quanteda&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Language detection: &lt;code&gt;cld2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Spell-checking: &lt;code&gt;hunspell&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
