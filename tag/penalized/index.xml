<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>penalized | Michal Ovádek</title>
    <link>https://michalovadek.github.io/tag/penalized/</link>
      <atom:link href="https://michalovadek.github.io/tag/penalized/index.xml" rel="self" type="application/rss+xml" />
    <description>penalized</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Michal Ovádek</copyright><lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://michalovadek.github.io/media/icon_hu47f642fef15b30aa1926993bc56415fb_14840_512x512_fill_lanczos_center_3.png</url>
      <title>penalized</title>
      <link>https://michalovadek.github.io/tag/penalized/</link>
    </image>
    
    <item>
      <title>Grid search for elastic net regularization</title>
      <link>https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/grid-search-for-elastic-net-regularization/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/grid-search-for-elastic-net-regularization/</guid>
      <description>&lt;p&gt;This post is a footnote to documentation to the &lt;code&gt;glmnet&lt;/code&gt; package and the &lt;code&gt;tidymodels&lt;/code&gt; framework. &lt;code&gt;glmnet&lt;/code&gt; is best known for fitting models via penalized maximum likelihood like ridge, lasso and elastic net regression. As explained in its documentatiom, &lt;code&gt;glmnet&lt;/code&gt; solves the problem&lt;/p&gt;
&lt;p&gt;$\begin{align}
\min_{\beta_0,\beta_1}\frac{1}{N}\sum_{i=1}^Nw_il(y_i,\beta_0+β^Tx_i)+\lambda[(1−\alpha)∥\beta∥^2_2/2+\alpha∥\beta∥_1]
\end{align}$&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;glmnet::glmnet()&lt;/code&gt; conducts a grid search over values of &lt;em&gt;$\lambda$&lt;/em&gt; which controls the overall strength of the penalty in the second term. When &lt;em&gt;$\alpha = 1$&lt;/em&gt; we speak of lasso regression which can shrink coefficients to zero (discard them), while ridge regression ($\alpha = 0$) does not remove features.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;glmnet::glmnet()&lt;/code&gt; expects the user to set the value of &lt;em&gt;$\alpha \in [0,1]$&lt;/em&gt;. In practice, however, many users will not know whether ridge, lasso or their mixture is most appropriate to their problem. Especially when we only care about minimizing &lt;em&gt;$|\hat{y} - y|$&lt;/em&gt; we might want to conduct a grid search over both &lt;em&gt;$\lambda$&lt;/em&gt; and &lt;em&gt;$\alpha$&lt;/em&gt; to find the most suitable tuple of hyperparameter values for our model. The example below shows that this dual search is particularly easy to implement with &lt;code&gt;tidymodels&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;house-prices-in-slovakia&#34;&gt;House prices in Slovakia&lt;/h2&gt;
&lt;p&gt;To exemplify the grid search I use a toy dataset with possible determinants of average quarterly house prices ($y$) in Slovakia. Let&amp;rsquo;s pretend that we have no idea about what affects house prices. Our design matrix &lt;em&gt;$X$&lt;/em&gt; consists of a bunch of more or less reasonable predictors like average wage and net migration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load the data
data &amp;lt;- &amp;quot;https://github.com/michalovadek/misc/blob/main/sk_house_prices.RData?raw=true&amp;quot;
load(url(data))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will need the &lt;code&gt;tidymodels&lt;/code&gt; set of packages and a seed for replicability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidymodels)
#require(glmnet)

set.seed(94241)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our goal is to find a penalized regression model that is good at predicting house prices with the available predictors. To mitigate overfitting, we will both split our data into a training and a test set and deploy k-fold cross-validation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# let&#39;s not worry about missing values and time ids
data &amp;lt;- dat_agg |&amp;gt; 
  dplyr::select(-year,-quarter) |&amp;gt; 
  tidyr::fill(dplyr::everything(), .direction = &amp;quot;updown&amp;quot;)

# we keep 1/4 of the observations for testing
split &amp;lt;- initial_split(data,
                       prop = 3/4)

train_data &amp;lt;- training(split)
test_data &amp;lt;- testing(split)

# k-fold cross-validation
folds &amp;lt;- vfold_cv(train_data, v = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We define the model and workflow for fitting it. We will be conducting a grid search over both &lt;code&gt;penalty&lt;/code&gt; ($\lambda$) and &lt;code&gt;mixture&lt;/code&gt; ($\alpha$).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define model
model &amp;lt;- linear_reg(penalty = tune(), mixture = tune()) |&amp;gt;
  set_engine(&amp;quot;glmnet&amp;quot;)

# RHS in formula selects all predictors
model_wf &amp;lt;- workflow() |&amp;gt;
  add_model(model) |&amp;gt;
  add_formula(price ~ .)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the grid search. We are looking for the best combination of &lt;em&gt;$\lambda$&lt;/em&gt; and &lt;em&gt;$\alpha$&lt;/em&gt;. We create this grid with the appropriately named &lt;code&gt;expand_grid()&lt;/code&gt; (or &lt;code&gt;expand.grid&lt;/code&gt; in &lt;code&gt;base&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# suitable values of lambda will vary
# you can run glmnet::glmnet() to get the ballpark range for your data
grid &amp;lt;- tidyr::expand_grid(penalty = seq(0.1, 15, length.out = 50),
                           mixture = c(0, 0.33, 0.66, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are ready to search the hyperparameter space using our k-folded training data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# we will use R^2 to evaluate model performance
search_res &amp;lt;- model_wf |&amp;gt; 
  tune_grid(folds,
            grid = grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rsq))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results can be parsed manually by running &lt;code&gt;collect_metrics()&lt;/code&gt; but &lt;code&gt;tidymodels&lt;/code&gt; comes with a handy &lt;code&gt;autoplot()&lt;/code&gt; method that will give us a quick answer as to which hyperparameter tuple performed best.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;autoplot(search_res)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/2021-12-05-grid-search-for-elastic-net-regularization_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can immediately see some clear differences between the model options. For the selected range of &lt;em&gt;$\lambda$&lt;/em&gt;, the performance of ridge regression does not change. Given how small our toy dataset is (and $N_{obs} &amp;gt; N_{predictors}$), it is not surprising that small penalties perform best.&lt;/p&gt;
&lt;p&gt;The workflow is completed by picking and fitting the best model and seeing how it performs on unseen data (our test set).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# select model by highest R^2
best_param &amp;lt;- search_res |&amp;gt; select_best()

# final workflow
model_wf_final &amp;lt;- finalize_workflow(model_wf, best_param)

# fit final model
final_fit &amp;lt;- model_wf_final |&amp;gt; 
  fit(data = train_data)

# predict using test set
test_data$.pred &amp;lt;- predict(final_fit, new_data = test_data)$.pred
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot y and yhat
test_data |&amp;gt; 
  ggplot(aes(x = price, y = .pred)) +
  geom_point() +
  geom_abline(slope = 1, color = &amp;quot;grey30&amp;quot;) +
  theme_minimal()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://michalovadek.github.io/post/2021-12-05-grid-search-for-elastic-net-regularization/2021-12-05-grid-search-for-elastic-net-regularization_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Looks good. Final performance metrics are obtained via &lt;code&gt;metrics()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;metrics(test_data, truth = price, estimate = .pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rmse    standard      72.5  
## 2 rsq     standard       0.938
## 3 mae     standard      47.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it. &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;tidymodels&lt;/code&gt; are a powerful and easy-to-use duo for high-dimensional data analysis.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
